{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bfortuner/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1401: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torch.autograd as autograd\n",
    "\n",
    "import imp\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils.training as train_utils; imp.reload(train_utils)\n",
    "import utils.plot as plot_utils; imp.reload(plot_utils)\n",
    "\n",
    "import camvid_dataset as camvid\n",
    "import joint_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH='data/'\n",
    "RESULTS_PATH='results/'\n",
    "WEIGHTS_PATH='models/'\n",
    "CAMVID_PATH=DATA_PATH+'CamVid/'\n",
    "PROJECT_NAME='tiramisu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FirstConvLayer**\n",
    "\n",
    "* 3x3 Conv2D (pad=, stride=, in_chans=3, out_chans=48)\n",
    "\n",
    "**DenseLayer**\n",
    "\n",
    "* BatchNorm\n",
    "* ReLU\n",
    "* 3x3 Conv2d (pad=, stride=, in_chans=, out_chans=) - \"no resolution loss\" - padding included\n",
    "* Dropout (.2)\n",
    "\n",
    "**DenseBlock**\n",
    "\n",
    "* Input = FirstConvLayer, TransitionDown, or TransitionUp\n",
    "* Loop to create L DenseLayers (L=n_layers)\n",
    "* On TransitionDown we Concat(Input, FinalDenseLayerActivation)\n",
    "* On TransitionUp we do not Concat with input, instead pass FinalDenseLayerActivation to TransitionUp block\n",
    "\n",
    "**TransitionDown**\n",
    "\n",
    "* BatchNorm\n",
    "* ReLU\n",
    "* 1x1 Conv2D (pad=, stride=, in_chans=, out_chans=)\n",
    "* Dropout (0.2)\n",
    "* 2x2 MaxPooling\n",
    "\n",
    "**Bottleneck**\n",
    "\n",
    "* DenseBlock (15 layers)\n",
    "\n",
    "**TransitionUp**\n",
    "\n",
    "* 3x3 Transposed Convolution (pad=, stride=2, in_chans=, out_chans=)\n",
    "* Concat(PreviousDenseBlock, SkipConnection) - from cooresponding DenseBlock on transition down\n",
    "\n",
    "**FinalBlock**\n",
    "\n",
    "* 1x1 Conv2d (pad=, stride=, in_chans=256, out_chans=n_classes)\n",
    "* Softmax\n",
    "\n",
    "**FCDenseNet103 Architecture**\n",
    "\n",
    "* input (in_chans=3 for RGB)\n",
    "* 3x3 ConvLayer (out_chans=48)\n",
    "* DB (4 layers) + TD\n",
    "* DB (5 layers) + TD\n",
    "* DB (7 layers) + TD\n",
    "* DB (10 layers) + TD\n",
    "* DB (12 layers) + TD\n",
    "* Bottleneck (15 layers)\n",
    "* TU + DB (12 layers)\n",
    "* TU + DB (10 layers)\n",
    "* TU + DB (7 layers)\n",
    "* TU + DB (5 layers)\n",
    "* TU + DB (4 layers)\n",
    "* 1x1 ConvLayer (out_chans=n_classes) n_classes=11 for CamVid\n",
    "* Softmax\n",
    "\n",
    "**FCDenseNet67**\n",
    "\n",
    "* GrowthRate (k) = 16\n",
    "* 5 layers per dense block\n",
    "* 1 Conv Layer\n",
    "* 5 DenseBlocks Downsample (25 layers)\n",
    "* 5 TransitionDown\n",
    "* 5 Bottleneck layers\n",
    "* 5 Dense Blocks Upsample (25 layers)\n",
    "* 5 TransitionUp\n",
    "* 1 Conv Layer\n",
    "* 1 Softmax layer (doesn't count)\n",
    "67 Total layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DenseLayer(nn.Sequential):\n",
    "    def __init__(self, in_channels, growth_rate):\n",
    "        super(DenseLayer, self).__init__()\n",
    "        self.add_module('norm', nn.BatchNorm2d(num_features=in_channels))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "        \n",
    "        #author's impl - lasange 'same' pads with half \n",
    "        # filter size (rounded down) on \"both\" sides\n",
    "        self.add_module('conv', nn.Conv2d(in_channels=in_channels, \n",
    "                out_channels=growth_rate, kernel_size=3, stride=1, \n",
    "                  padding=1, bias=True))\n",
    "        \n",
    "        self.add_module('drop', nn.Dropout2d(0.2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return super(DenseLayer, self).forward(x)\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate, n_layers, upsample=False):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.upsample = upsample\n",
    "        self.layers = nn.ModuleList([DenseLayer(\n",
    "            in_channels + i*growth_rate, growth_rate)\n",
    "            for i in range(n_layers)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.upsample:\n",
    "            new_features = []\n",
    "            #we pass all previous activations into each dense layer normally\n",
    "            #But we only store each dense layer's output in the new_features array\n",
    "            for layer in self.layers:\n",
    "                out = layer(x)\n",
    "                x = torch.cat([x, out], 1)\n",
    "                new_features.append(out)\n",
    "            return torch.cat(new_features,1)\n",
    "        else:\n",
    "            for layer in self.layers:\n",
    "                out = layer(x)\n",
    "                x = torch.cat([x, out], 1) # 1 = channel axis\n",
    "            return x \n",
    "    \n",
    "class TransitionDown(nn.Sequential):\n",
    "    def __init__(self, in_channels):\n",
    "        super(TransitionDown, self).__init__()\n",
    "        self.add_module('norm', nn.BatchNorm2d(num_features=in_channels))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "        self.add_module('conv', nn.Conv2d(in_channels=in_channels, \n",
    "              out_channels=in_channels, kernel_size=1, stride=1, \n",
    "                padding=0, bias=True))\n",
    "        self.add_module('drop', nn.Dropout2d(0.2))\n",
    "        self.add_module('maxpool', nn.MaxPool2d(2))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return super(TransitionDown, self).forward(x)\n",
    "    \n",
    "class TransitionUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(TransitionUp, self).__init__()\n",
    "        self.convTrans = nn.ConvTranspose2d(in_channels=in_channels, \n",
    "               out_channels=out_channels, kernel_size=3, stride=2, \n",
    "                padding=0, bias=True) #crop = 'valid' means padding=0. Padding has reverse effect for transpose conv (reduces output size)\n",
    "        #http://lasagne.readthedocs.io/en/latest/modules/layers/conv.html#lasagne.layers.TransposedConv2DLayer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.convTrans(x)\n",
    "    \n",
    "class Bottleneck(nn.Sequential):\n",
    "    def __init__(self, in_channels, growth_rate, n_layers):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.add_module('bottleneck', DenseBlock(in_channels, growth_rate, n_layers, upsample=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return super(Bottleneck, self).forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def center_crop(layer, target_width):\n",
    "    #Assumes square\n",
    "    batch_size, n_channels, layer_width, layer_height = layer.size()\n",
    "    xy1 = (layer_width - target_width) // 2\n",
    "    return layer[:, :, xy1:(xy1 + target_width), xy1:(xy1 + target_width)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FCDenseNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, n_blocks=5, layers_per_block=5, growth_rate=16, \n",
    "                 out_chans_first_conv=48, n_classes=12):\n",
    "        super(FCDenseNet, self).__init__()\n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_channels = 0\n",
    "        self.skipNumChannels = []\n",
    "        \n",
    "        \n",
    "        #####################\n",
    "        # First Convolution #\n",
    "        #####################\n",
    "\n",
    "        self.add_module('firstconv', nn.Conv2d(in_channels=in_channels, \n",
    "                  out_channels=out_chans_first_conv, kernel_size=3, \n",
    "                  stride=1, padding=1, bias=True))\n",
    "        self.n_channels = out_chans_first_conv\n",
    "        \n",
    "        \n",
    "        \n",
    "        #####################\n",
    "        # Downsampling path #\n",
    "        #####################\n",
    "        \n",
    "        self.denseBlocksDown = nn.ModuleList([])\n",
    "        self.transDownBlocks = nn.ModuleList([])\n",
    "        for i in range(n_blocks):\n",
    "            self.denseBlocksDown.append(\n",
    "                DenseBlock(self.n_channels, growth_rate, layers_per_block))\n",
    "            self.n_channels += (growth_rate*layers_per_block)\n",
    "            #print(\"nchannelsdensedown%d: %d\" % (i, self.n_channels))\n",
    "            self.skipNumChannels.insert(0,self.n_channels)\n",
    "            self.transDownBlocks.append(TransitionDown(self.n_channels))\n",
    "            \n",
    "            \n",
    "            \n",
    "        #####################\n",
    "        #     Bottleneck    #\n",
    "        #####################\n",
    "        \n",
    "        self.add_module('bottleneck',Bottleneck(self.n_channels, \n",
    "                                     growth_rate, layers_per_block))\n",
    "        prev_block_channels = growth_rate*layers_per_block\n",
    "        self.n_channels += prev_block_channels \n",
    "        #print(\"bottleneck: %d\" % (self.n_channels))\n",
    "        \n",
    "        \n",
    "        \n",
    "        #######################\n",
    "        #   Upsampling path   #\n",
    "        #######################\n",
    "\n",
    "        self.transUpBlocks = nn.ModuleList([])\n",
    "        self.denseBlocksUp = nn.ModuleList([])\n",
    "        for i in range(n_blocks-1):\n",
    "            self.transUpBlocks.append(TransitionUp(\n",
    "                growth_rate*layers_per_block, growth_rate*layers_per_block))\n",
    "            self.n_channels = prev_block_channels + self.skipNumChannels[i]\n",
    "            #print(\"nchannelsTU%d: %d\" % (i, self.n_channels))\n",
    "            self.denseBlocksUp.append(DenseBlock(\n",
    "                self.n_channels, growth_rate, layers_per_block, \n",
    "                    upsample=True))\n",
    "            prev_block_channels = growth_rate*layers_per_block\n",
    "            self.n_channels += prev_block_channels\n",
    "            #print(\"nchannelsdensedown%d: %d\" % (i, self.n_channels))\n",
    "            \n",
    "        #One final normal dense block\n",
    "        self.transUpBlocks.append(TransitionUp(\n",
    "            growth_rate*layers_per_block, growth_rate*layers_per_block))\n",
    "        self.n_channels = prev_block_channels + self.skipNumChannels[-1]\n",
    "        #print(\"nchannelsTU%d: %d\" % (i, self.n_channels))\n",
    "        self.denseBlocksUp.append(DenseBlock(\n",
    "            self.n_channels, growth_rate, layers_per_block, \n",
    "                upsample=False))\n",
    "        self.n_channels += growth_rate*layers_per_block\n",
    "\n",
    "        \n",
    "        \n",
    "        #####################\n",
    "        #      Softmax      #\n",
    "        #####################\n",
    "\n",
    "        self.finalConv = nn.Conv2d(in_channels=self.n_channels, \n",
    "               out_channels=n_classes, kernel_size=1, stride=1, \n",
    "                   padding=0, bias=True)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(\"INPUT SIZE\",x.size())\n",
    "        out = self.firstconv(x)\n",
    "        \n",
    "        skip_connections = []\n",
    "        for i in range(self.n_blocks):\n",
    "            out = self.denseBlocksDown[i](out)\n",
    "            skip_connections.append(out)\n",
    "            out = self.transDownBlocks[i](out)\n",
    "            \n",
    "        out = self.bottleneck(out)\n",
    "        print(\"BOTTLESIZE\", out.size())\n",
    "        \n",
    "        for i in range(self.n_blocks):\n",
    "            out = self.transUpBlocks[i](out)\n",
    "            skip = skip_connections.pop()\n",
    "            out = center_crop(out, skip.size()[2])\n",
    "            out = torch.cat([out, skip], 1)\n",
    "            print(skip.size(), out.size())\n",
    "            out = self.denseBlocksUp[i](out)\n",
    "            print (\"DENSEOUT\", out.size())\n",
    "            \n",
    "        out = self.finalConv(out)\n",
    "        print(\"LASTCONV\", out.size())\n",
    "        # Reshape\n",
    "        bs, c, h, w = out.size()\n",
    "        out = out.view(bs*h*w, c)\n",
    "        \n",
    "        out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FCDenseNet (\n",
      "  (firstconv): Conv2d(3, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (denseBlocksDown): ModuleList (\n",
      "    (0): DenseBlock (\n",
      "      (layers): ModuleList (\n",
      "        (0): DenseLayer (\n",
      "          (norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(48, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (1): DenseLayer (\n",
      "          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (2): DenseLayer (\n",
      "          (norm): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(80, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (3): DenseLayer (\n",
      "          (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(96, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (4): DenseLayer (\n",
      "          (norm): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(112, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): DenseBlock (\n",
      "      (layers): ModuleList (\n",
      "        (0): DenseLayer (\n",
      "          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (1): DenseLayer (\n",
      "          (norm): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(144, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (2): DenseLayer (\n",
      "          (norm): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(160, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (3): DenseLayer (\n",
      "          (norm): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(176, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (4): DenseLayer (\n",
      "          (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(192, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): DenseBlock (\n",
      "      (layers): ModuleList (\n",
      "        (0): DenseLayer (\n",
      "          (norm): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(208, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (1): DenseLayer (\n",
      "          (norm): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(224, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (2): DenseLayer (\n",
      "          (norm): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(240, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (3): DenseLayer (\n",
      "          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (4): DenseLayer (\n",
      "          (norm): BatchNorm2d(272, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(272, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): DenseBlock (\n",
      "      (layers): ModuleList (\n",
      "        (0): DenseLayer (\n",
      "          (norm): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(288, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (1): DenseLayer (\n",
      "          (norm): BatchNorm2d(304, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(304, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (2): DenseLayer (\n",
      "          (norm): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(320, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (3): DenseLayer (\n",
      "          (norm): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(336, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (4): DenseLayer (\n",
      "          (norm): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(352, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): DenseBlock (\n",
      "      (layers): ModuleList (\n",
      "        (0): DenseLayer (\n",
      "          (norm): BatchNorm2d(368, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(368, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (1): DenseLayer (\n",
      "          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(384, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (2): DenseLayer (\n",
      "          (norm): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(400, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (3): DenseLayer (\n",
      "          (norm): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(416, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (4): DenseLayer (\n",
      "          (norm): BatchNorm2d(432, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(432, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (transDownBlocks): ModuleList (\n",
      "    (0): TransitionDown (\n",
      "      (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (drop): Dropout2d (p=0.2)\n",
      "      (maxpool): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "    )\n",
      "    (1): TransitionDown (\n",
      "      (norm): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv): Conv2d(208, 208, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (drop): Dropout2d (p=0.2)\n",
      "      (maxpool): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "    )\n",
      "    (2): TransitionDown (\n",
      "      (norm): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv): Conv2d(288, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (drop): Dropout2d (p=0.2)\n",
      "      (maxpool): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "    )\n",
      "    (3): TransitionDown (\n",
      "      (norm): BatchNorm2d(368, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv): Conv2d(368, 368, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (drop): Dropout2d (p=0.2)\n",
      "      (maxpool): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "    )\n",
      "    (4): TransitionDown (\n",
      "      (norm): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv): Conv2d(448, 448, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (drop): Dropout2d (p=0.2)\n",
      "      (maxpool): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (bottleneck): Bottleneck (\n",
      "    (bottleneck): DenseBlock (\n",
      "      (layers): ModuleList (\n",
      "        (0): DenseLayer (\n",
      "          (norm): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(448, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (1): DenseLayer (\n",
      "          (norm): BatchNorm2d(464, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(464, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (2): DenseLayer (\n",
      "          (norm): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(480, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (3): DenseLayer (\n",
      "          (norm): BatchNorm2d(496, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(496, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (4): DenseLayer (\n",
      "          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (transUpBlocks): ModuleList (\n",
      "    (0): TransitionUp (\n",
      "      (convTrans): ConvTranspose2d(80, 80, kernel_size=(3, 3), stride=(2, 2))\n",
      "    )\n",
      "    (1): TransitionUp (\n",
      "      (convTrans): ConvTranspose2d(80, 80, kernel_size=(3, 3), stride=(2, 2))\n",
      "    )\n",
      "    (2): TransitionUp (\n",
      "      (convTrans): ConvTranspose2d(80, 80, kernel_size=(3, 3), stride=(2, 2))\n",
      "    )\n",
      "    (3): TransitionUp (\n",
      "      (convTrans): ConvTranspose2d(80, 80, kernel_size=(3, 3), stride=(2, 2))\n",
      "    )\n",
      "    (4): TransitionUp (\n",
      "      (convTrans): ConvTranspose2d(80, 80, kernel_size=(3, 3), stride=(2, 2))\n",
      "    )\n",
      "  )\n",
      "  (denseBlocksUp): ModuleList (\n",
      "    (0): DenseBlock (\n",
      "      (layers): ModuleList (\n",
      "        (0): DenseLayer (\n",
      "          (norm): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(528, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (1): DenseLayer (\n",
      "          (norm): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(544, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (2): DenseLayer (\n",
      "          (norm): BatchNorm2d(560, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(560, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (3): DenseLayer (\n",
      "          (norm): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(576, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (4): DenseLayer (\n",
      "          (norm): BatchNorm2d(592, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(592, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): DenseBlock (\n",
      "      (layers): ModuleList (\n",
      "        (0): DenseLayer (\n",
      "          (norm): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(448, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (1): DenseLayer (\n",
      "          (norm): BatchNorm2d(464, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(464, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (2): DenseLayer (\n",
      "          (norm): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(480, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (3): DenseLayer (\n",
      "          (norm): BatchNorm2d(496, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(496, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (4): DenseLayer (\n",
      "          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): DenseBlock (\n",
      "      (layers): ModuleList (\n",
      "        (0): DenseLayer (\n",
      "          (norm): BatchNorm2d(368, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(368, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (1): DenseLayer (\n",
      "          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(384, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (2): DenseLayer (\n",
      "          (norm): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(400, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (3): DenseLayer (\n",
      "          (norm): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(416, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (4): DenseLayer (\n",
      "          (norm): BatchNorm2d(432, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(432, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): DenseBlock (\n",
      "      (layers): ModuleList (\n",
      "        (0): DenseLayer (\n",
      "          (norm): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(288, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (1): DenseLayer (\n",
      "          (norm): BatchNorm2d(304, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(304, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (2): DenseLayer (\n",
      "          (norm): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(320, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (3): DenseLayer (\n",
      "          (norm): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(336, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (4): DenseLayer (\n",
      "          (norm): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(352, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): DenseBlock (\n",
      "      (layers): ModuleList (\n",
      "        (0): DenseLayer (\n",
      "          (norm): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(208, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (1): DenseLayer (\n",
      "          (norm): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(224, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (2): DenseLayer (\n",
      "          (norm): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(240, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (3): DenseLayer (\n",
      "          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "        (4): DenseLayer (\n",
      "          (norm): BatchNorm2d(272, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (conv): Conv2d(272, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (drop): Dropout2d (p=0.2)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (finalConv): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (softmax): Softmax ()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = FCDenseNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "* https://github.com/SimJeg/FC-DenseNet/blob/master/config/FC-DenseNet103.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_crop_size = (224, 224) # None for full size\n",
    "n_classes = 12 #11 + background\n",
    "\n",
    "# Training\n",
    "seed = 0\n",
    "learning_rate = .001\n",
    "lr_sched_decay = 0.995 # Applied each epoch\n",
    "weight_decay = 0.0001\n",
    "num_epochs = 750\n",
    "max_patience = 150\n",
    "loss_function = 'crossentropy'\n",
    "optimizer = 'rmsprop' # Consider adam for training on other dataset, or decrease epsilon to 1e-12\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        #kaiming is first name of author whose last name is 'He' lol\n",
    "        init.kaiming_uniform(m.weight) \n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "* https://github.com/SimJeg/FC-DenseNet/blob/master/data_loader.py\n",
    "* https://github.com/pytorch/vision/pull/90\n",
    "* https://github.com/SimJeg/FC-DenseNet/issues/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "traindir = os.path.join(CAMVID_PATH, 'train')\n",
    "valdir = os.path.join(CAMVID_PATH, 'val')\n",
    "testdir = os.path.join(CAMVID_PATH, 'test')\n",
    "\n",
    "normalize = transforms.Normalize(mean=camvid.mean, std=camvid.std)\n",
    "train_joint_transformer = transforms.Compose([\n",
    "    joint_transforms.JointRandomCrop(224),\n",
    "    joint_transforms.JointRandomHorizontalFlip()\n",
    "    ])\n",
    "train_dset = camvid.CamVid(CAMVID_PATH, 'train',\n",
    "      joint_transform=train_joint_transformer,\n",
    "      transform=transforms.Compose([\n",
    "          transforms.ToTensor(),\n",
    "          normalize,\n",
    "    ]))\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dset = camvid.CamVid(\n",
    "    CAMVID_PATH, 'val', joint_transform=None,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]))\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dset = camvid.CamVid(\n",
    "    CAMVID_PATH, 'test', joint_transform=None,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]))\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainImages: 367\n",
      "ValImages: 101\n",
      "TestImages: 233\n",
      "NumClasses: 12\n"
     ]
    }
   ],
   "source": [
    "# print(train_loader.dataset.classes)\n",
    "# print(train_loader.dataset.class_weight)\n",
    "# print(train_loader.dataset.imgs[:3])\n",
    "# print(train_loader.dataset.mean)\n",
    "# print(train_loader.dataset.std)\n",
    "print(\"TrainImages: %d\" %len(train_loader.dataset.imgs))\n",
    "print(\"ValImages: %d\" %len(val_loader.dataset.imgs))\n",
    "print(\"TestImages: %d\" %len(test_loader.dataset.imgs))\n",
    "print(\"NumClasses: %d\" % len(train_loader.dataset.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "2.145308017730713\n",
      "-1.5297001600265503\n",
      "torch.Size([224, 224])\n",
      "11\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = next(iter(train_loader))\n",
    "t = targets[0]\n",
    "i = inputs[0]\n",
    "\n",
    "#Inputs are tensors of normalized pixel values\n",
    "print (inputs[0].size())\n",
    "print(i.max())\n",
    "print(i.min())\n",
    "\n",
    "#Targets are tensors of class labels from 0-11 (0 means background)\n",
    "print(targets[0].size())\n",
    "print(t.max())\n",
    "print(t.min())\n",
    "\n",
    "#Reshape Target To Match Output of Model\n",
    "#360*480 = 172800\n",
    "#targets[:, 0].size()\n",
    "values, indices = t.cpu().max(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Test Model Output\n",
    "model = FCDenseNet(in_channels=3, n_blocks=5, layers_per_block=5, growth_rate=16, \n",
    "                 out_chans_first_conv=48, n_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FCDenseNet (\n",
       "  (firstconv): Conv2d(3, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (denseBlocksDown): ModuleList (\n",
       "    (0): DenseBlock (\n",
       "      (layers): ModuleList (\n",
       "        (0): DenseLayer (\n",
       "          (norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(48, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (1): DenseLayer (\n",
       "          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (2): DenseLayer (\n",
       "          (norm): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(80, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (3): DenseLayer (\n",
       "          (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(96, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (4): DenseLayer (\n",
       "          (norm): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(112, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): DenseBlock (\n",
       "      (layers): ModuleList (\n",
       "        (0): DenseLayer (\n",
       "          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (1): DenseLayer (\n",
       "          (norm): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(144, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (2): DenseLayer (\n",
       "          (norm): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(160, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (3): DenseLayer (\n",
       "          (norm): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(176, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (4): DenseLayer (\n",
       "          (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(192, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): DenseBlock (\n",
       "      (layers): ModuleList (\n",
       "        (0): DenseLayer (\n",
       "          (norm): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(208, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (1): DenseLayer (\n",
       "          (norm): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(224, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (2): DenseLayer (\n",
       "          (norm): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(240, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (3): DenseLayer (\n",
       "          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (4): DenseLayer (\n",
       "          (norm): BatchNorm2d(272, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(272, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): DenseBlock (\n",
       "      (layers): ModuleList (\n",
       "        (0): DenseLayer (\n",
       "          (norm): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(288, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (1): DenseLayer (\n",
       "          (norm): BatchNorm2d(304, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(304, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (2): DenseLayer (\n",
       "          (norm): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(320, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (3): DenseLayer (\n",
       "          (norm): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(336, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (4): DenseLayer (\n",
       "          (norm): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(352, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): DenseBlock (\n",
       "      (layers): ModuleList (\n",
       "        (0): DenseLayer (\n",
       "          (norm): BatchNorm2d(368, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(368, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (1): DenseLayer (\n",
       "          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(384, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (2): DenseLayer (\n",
       "          (norm): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(400, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (3): DenseLayer (\n",
       "          (norm): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(416, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (4): DenseLayer (\n",
       "          (norm): BatchNorm2d(432, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(432, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transDownBlocks): ModuleList (\n",
       "    (0): TransitionDown (\n",
       "      (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU (inplace)\n",
       "      (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (drop): Dropout2d (p=0.2)\n",
       "      (maxpool): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    )\n",
       "    (1): TransitionDown (\n",
       "      (norm): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU (inplace)\n",
       "      (conv): Conv2d(208, 208, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (drop): Dropout2d (p=0.2)\n",
       "      (maxpool): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    )\n",
       "    (2): TransitionDown (\n",
       "      (norm): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU (inplace)\n",
       "      (conv): Conv2d(288, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (drop): Dropout2d (p=0.2)\n",
       "      (maxpool): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    )\n",
       "    (3): TransitionDown (\n",
       "      (norm): BatchNorm2d(368, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU (inplace)\n",
       "      (conv): Conv2d(368, 368, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (drop): Dropout2d (p=0.2)\n",
       "      (maxpool): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    )\n",
       "    (4): TransitionDown (\n",
       "      (norm): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU (inplace)\n",
       "      (conv): Conv2d(448, 448, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (drop): Dropout2d (p=0.2)\n",
       "      (maxpool): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (bottleneck): Bottleneck (\n",
       "    (bottleneck): DenseBlock (\n",
       "      (layers): ModuleList (\n",
       "        (0): DenseLayer (\n",
       "          (norm): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(448, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (1): DenseLayer (\n",
       "          (norm): BatchNorm2d(464, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(464, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (2): DenseLayer (\n",
       "          (norm): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(480, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (3): DenseLayer (\n",
       "          (norm): BatchNorm2d(496, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(496, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (4): DenseLayer (\n",
       "          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transUpBlocks): ModuleList (\n",
       "    (0): TransitionUp (\n",
       "      (convTrans): ConvTranspose2d(80, 80, kernel_size=(3, 3), stride=(2, 2))\n",
       "    )\n",
       "    (1): TransitionUp (\n",
       "      (convTrans): ConvTranspose2d(80, 80, kernel_size=(3, 3), stride=(2, 2))\n",
       "    )\n",
       "    (2): TransitionUp (\n",
       "      (convTrans): ConvTranspose2d(80, 80, kernel_size=(3, 3), stride=(2, 2))\n",
       "    )\n",
       "    (3): TransitionUp (\n",
       "      (convTrans): ConvTranspose2d(80, 80, kernel_size=(3, 3), stride=(2, 2))\n",
       "    )\n",
       "    (4): TransitionUp (\n",
       "      (convTrans): ConvTranspose2d(80, 80, kernel_size=(3, 3), stride=(2, 2))\n",
       "    )\n",
       "  )\n",
       "  (denseBlocksUp): ModuleList (\n",
       "    (0): DenseBlock (\n",
       "      (layers): ModuleList (\n",
       "        (0): DenseLayer (\n",
       "          (norm): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(528, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (1): DenseLayer (\n",
       "          (norm): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(544, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (2): DenseLayer (\n",
       "          (norm): BatchNorm2d(560, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(560, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (3): DenseLayer (\n",
       "          (norm): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(576, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (4): DenseLayer (\n",
       "          (norm): BatchNorm2d(592, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(592, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): DenseBlock (\n",
       "      (layers): ModuleList (\n",
       "        (0): DenseLayer (\n",
       "          (norm): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(448, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (1): DenseLayer (\n",
       "          (norm): BatchNorm2d(464, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(464, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (2): DenseLayer (\n",
       "          (norm): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(480, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (3): DenseLayer (\n",
       "          (norm): BatchNorm2d(496, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(496, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (4): DenseLayer (\n",
       "          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): DenseBlock (\n",
       "      (layers): ModuleList (\n",
       "        (0): DenseLayer (\n",
       "          (norm): BatchNorm2d(368, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(368, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (1): DenseLayer (\n",
       "          (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(384, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (2): DenseLayer (\n",
       "          (norm): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(400, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (3): DenseLayer (\n",
       "          (norm): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(416, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (4): DenseLayer (\n",
       "          (norm): BatchNorm2d(432, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(432, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): DenseBlock (\n",
       "      (layers): ModuleList (\n",
       "        (0): DenseLayer (\n",
       "          (norm): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(288, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (1): DenseLayer (\n",
       "          (norm): BatchNorm2d(304, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(304, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (2): DenseLayer (\n",
       "          (norm): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(320, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (3): DenseLayer (\n",
       "          (norm): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(336, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (4): DenseLayer (\n",
       "          (norm): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(352, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): DenseBlock (\n",
       "      (layers): ModuleList (\n",
       "        (0): DenseLayer (\n",
       "          (norm): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(208, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (1): DenseLayer (\n",
       "          (norm): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(224, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (2): DenseLayer (\n",
       "          (norm): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(240, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (3): DenseLayer (\n",
       "          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "        (4): DenseLayer (\n",
       "          (norm): BatchNorm2d(272, eps=1e-05, momentum=0.1, affine=True)\n",
       "          (relu): ReLU (inplace)\n",
       "          (conv): Conv2d(272, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (drop): Dropout2d (p=0.2)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (finalConv): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (softmax): Softmax ()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT SIZE torch.Size([1, 3, 224, 224])\n",
      "BOTTLESIZE torch.Size([1, 80, 7, 7])\n",
      "torch.Size([1, 448, 14, 14]) torch.Size([1, 528, 14, 14])\n",
      "DENSEOUT torch.Size([1, 80, 14, 14])\n",
      "torch.Size([1, 368, 28, 28]) torch.Size([1, 448, 28, 28])\n",
      "DENSEOUT torch.Size([1, 80, 28, 28])\n",
      "torch.Size([1, 288, 56, 56]) torch.Size([1, 368, 56, 56])\n",
      "DENSEOUT torch.Size([1, 80, 56, 56])\n",
      "torch.Size([1, 208, 112, 112]) torch.Size([1, 288, 112, 112])\n",
      "DENSEOUT torch.Size([1, 80, 112, 112])\n",
      "torch.Size([1, 128, 224, 224]) torch.Size([1, 208, 224, 224])\n",
      "DENSEOUT torch.Size([1, 288, 224, 224])\n",
      "LASTCONV torch.Size([1, 12, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "pred = model(Variable(inputs.cuda()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tns = pred.data.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50176, 12])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize\n",
    "\n",
    "* https://discuss.pytorch.org/t/convert-pixel-wise-class-tensor-to-image-segmentation/1268"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Sky = [128,128,128]\n",
    "Building = [128,0,0]\n",
    "Pole = [192,192,128]\n",
    "#Road_marking = [255,69,0] ???\n",
    "Road = [128,64,128]\n",
    "Pavement = [60,40,222]\n",
    "Tree = [128,128,0]\n",
    "SignSymbol = [192,128,128]\n",
    "Fence = [64,64,128]\n",
    "Car = [64,0,128]\n",
    "Pedestrian = [64,64,0]\n",
    "Bicyclist = [0,128,192]\n",
    "Unlabelled = [0,0,0]\n",
    "\n",
    "label_colours = np.array([Sky, Building, Pole, Road, Pavement,\n",
    "      Tree, SignSymbol, Fence, Car, Pedestrian, Bicyclist, Unlabelled])\n",
    "\n",
    "def view_annotated(tensor, plot=True):\n",
    "    temp = tensor.numpy()\n",
    "    r = temp.copy()\n",
    "    g = temp.copy()\n",
    "    b = temp.copy()\n",
    "    for l in range(0,11):\n",
    "        r[temp==l]=label_colours[l,0]\n",
    "        g[temp==l]=label_colours[l,1]\n",
    "        b[temp==l]=label_colours[l,2]\n",
    "\n",
    "    rgb = np.zeros((temp.shape[0], temp.shape[1], 3))\n",
    "    rgb[:,:,0] = (r/255.0)#[:,:,0]\n",
    "    rgb[:,:,1] = (g/255.0)#[:,:,1]\n",
    "    rgb[:,:,2] = (b/255.0)#[:,:,2]\n",
    "    if plot:\n",
    "        plt.imshow(rgb)\n",
    "    else:\n",
    "        return rgb\n",
    "    \n",
    "def view_image(inp):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array(camvid.mean)\n",
    "    std = np.array(camvid.std)\n",
    "    inp = std * inp + mean\n",
    "    plt.imshow(inp)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get a batch of training data\n",
    "inputs, targets = next(iter(train_loader))\n",
    "#inputs, targets = next(iter(val_loader))\n",
    "#inputs, targets = next(iter(test_loader))\n",
    "\n",
    "# Plot Single Image\n",
    "view_image(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot Annotated Image\n",
    "view_annotated(targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot Grid of images\n",
    "out = torchvision.utils.make_grid(inputs, nrow=3)\n",
    "view_image(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "* https://github.com/pytorch/examples/blob/master/imagenet/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameters**\n",
    "\n",
    "* WeightInitialization = HeUniform\n",
    "* Optimizer = RMSProp\n",
    "* LR = .001 with exponential decay of 0.995 after each epoch\n",
    "* Data Augmentation = Random Crops, Vertical Flips\n",
    "* ValidationSet with early stopping based on IoU or MeanAccuracy with patience of 100 (50 during finetuning)\n",
    "* WeightDecay = .0001\n",
    "* Finetune with full-size images, LR = .0001\n",
    "* Dropout = 0.2\n",
    "* BatchNorm \"we use current batch stats at training, validation, and test time\"\n",
    "\n",
    "**CamVid**\n",
    "\n",
    "* TrainingSet = 367 frames\n",
    "* ValidationSet = 101 frames\n",
    "* TestSet = 233 frames\n",
    "* Images of resolution 360x480\n",
    "* Images \"Cropped\" to 224x224 for training --- center crop?\n",
    "* FullRes images used for finetuning\n",
    "* NumberOfClasses = 11 (output)\n",
    "* BatchSize = 3\n",
    "\n",
    "**FCDenseNet103**\n",
    "\n",
    "* GrowthRate = 16 (k, number of filters to each denselayer adds to the ever-growing concatenated output)\n",
    "* No pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        print (output.size(), target.size())\n",
    "        loss = criterion(output, target) #.view(-1,n_classes))\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "* https://github.com/ycszen/pytorch-ss/blob/master/loss.py\n",
    "* http://pytorch.org/docs/nn.html?highlight=logsoftmax#nllloss2d\n",
    "* https://github.com/SimJeg/FC-DenseNet/blob/master/metrics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def CrossEntropy2d(input, target, weight=None, size_average=False):\n",
    "    # input:(n, c, h, w) target:(n, h, w)\n",
    "    n, c, h, w = input.size()\n",
    "\n",
    "    input = input.transpose(1, 2).transpose(2, 3).contiguous()\n",
    "    input = input[target.view(n, h, w, 1).repeat(1, 1, 1, c) >= 0].view(-1, c)\n",
    "\n",
    "    target_mask = target >= 0\n",
    "    target = target[target_mask]\n",
    "    #loss = F.nll_loss(F.log_softmax(input), target, weight=weight, size_average=False)\n",
    "    loss = F.cross_entropy(input, target, weight=weight, size_average=False)\n",
    "    if size_average:\n",
    "        loss /= target_mask.sum().data[0]\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn.NLLLoss2d??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CrossEntropyLoss2d(nn.Module):\n",
    "    def __init__(self, weight=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.loss = nn.NLLLoss2d(weight)\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        return self.loss(F.log_softmax(outputs), targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 224, 224])\n",
      "torch.Size([3, 224, 224, 3])\n",
      "torch.Size([150528, 3])\n"
     ]
    }
   ],
   "source": [
    "n, c, h, w = inputs.size()\n",
    "print(inputs.size())\n",
    "rs = inputs.transpose(1, 2).transpose(2, 3).contiguous()\n",
    "print(rs.size())\n",
    "\n",
    "rs = rs[targets.view(n, h, w, 1).repeat(1, 1, 1, c) >= 0].view(-1, c)\n",
    "print(rs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([150528, 3])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FloatTensor' object has no attribute 'equals'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-4a4ffa6e1a72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'FloatTensor' object has no attribute 'equals'"
     ]
    }
   ],
   "source": [
    "n, c, h, w = inputs.size()\n",
    "out = inputs.view(n*h*w, c)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2d version?\n",
    "criterion = nn.CrossEntropyLoss(weight=camvid.class_weight.cuda()).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2d version?\n",
    "criterion = CrossEntropyLoss2d(weight=camvid.class_weight.cuda()).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs=1\n",
    "model = FCDenseNet(in_channels=3, n_blocks=5, layers_per_block=5, growth_rate=16, \n",
    "                 out_chans_first_conv=48, n_classes=n_classes)\n",
    "model = model.cuda()\n",
    "model.apply(weights_init)\n",
    "#cudnn.benchmark = True ??\n",
    "\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train(model, train_loader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, decay):\n",
    "    \"\"\"Decay LR by `decay` every 1 epochs\"\"\"\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    lr *= (decay ** (epoch // 1))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adjust patience\n",
    "# https://github.com/SimJeg/FC-DenseNet/blob/master/train.py#L176"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  + Number of params: 1059298\n",
      "Training new model from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train - Loss: 1.370707\tError: 43.750000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|| 1/1 [01:12<00:00, 72.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test - Loss: 1.3404, Error: 4590/10000 (46%)\n",
      "Time 1m 12s\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "existing_weights_fpath=None\n",
    "nEpochs=1\n",
    "\n",
    "net = FCDenseNet(in_channels=3, n_blocks=5, layers_per_block=5, growth_rate=16, \n",
    "                 out_chans_first_conv=48, n_classes=11)\n",
    "net = net.cuda()\n",
    "cudnn.benchmark = True\n",
    "\n",
    "optimizer = optim.RMSprop(net.parameters(), lr=.001, weight_decay=.0001)\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "print('  + Number of params: {}'.format(\n",
    "    sum([p.data.nelement() for p in net.parameters()])))\n",
    "\n",
    "if existing_weights_fpath:\n",
    "    startEpoch = train_utils.load_weights(net, existing_weights_fpath)\n",
    "    endEpoch = startEpoch + nEpochs\n",
    "    print ('Resume training at epoch: {}'.format(startEpoch))\n",
    "    if os.path.exists(RESULTS_PATH+'train.csv'): #assume test.csv exists\n",
    "        append_write = 'a' # append if already exists\n",
    "    else:\n",
    "        append_write = 'w' # make a new file if not\n",
    "    trainF = open(os.path.join(RESULTS_PATH, 'train.csv'), append_write)\n",
    "    testF = open(os.path.join(RESULTS_PATH, 'test.csv'), append_write)\n",
    "else:\n",
    "    print (\"Training new model from scratch\")\n",
    "    startEpoch = 1\n",
    "    endEpoch = nEpochs\n",
    "    trainF = open(os.path.join(RESULTS_PATH, 'train.csv'), 'w')\n",
    "    testF = open(os.path.join(RESULTS_PATH, 'test.csv'), 'w')\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(startEpoch, endEpoch+1)):\n",
    "    since = time.time()\n",
    "    train_utils.adjust_opt(\"sgd\", optimizer, epoch)\n",
    "    train_utils.train(epoch, net, trainLoader, optimizer, trainF)\n",
    "    train_utils.test(epoch, net, testLoader, optimizer, testF)\n",
    "    time_elapsed = time.time() - since  \n",
    "    print('Time {:.0f}m {:.0f}s\\n'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    if epoch != 1:\n",
    "        os.system('./utils/plot.py {} &'.format(RESULTS_PATH))\n",
    "\n",
    "trainF.close()\n",
    "testF.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## References\n",
    "\n",
    "* https://github.com/mattmacy/vnet.pytorch/blob/master/train.py\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "120px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
